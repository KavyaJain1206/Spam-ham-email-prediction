{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e8995-f8bc-4c5c-83d1-84507401f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"uciml/sms-spam-collection-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e38051-044c-4b31-8e48-cf9573b34dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder path\n",
    "dataset_dir = r\"C:\\Users\\kjain\\.cache\\kagglehub\\datasets\\uciml\\sms-spam-collection-dataset\\versions\\1\"\n",
    "dataset_path = os.path.join(dataset_dir, \"spam.csv\")\n",
    "\n",
    "# Load the CSV\n",
    "# The spam.csv from Kaggle usually has commas, but sometimes extra columns may exist, so we'll inspect first\n",
    "data = pd.read_csv(dataset_path, encoding='latin-1')  # encoding='latin-1' avoids special char issues\n",
    "\n",
    "# Optional: drop unnecessary extra columns\n",
    "# Most Kaggle spam.csv files have columns: 'v1', 'v2', 'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'\n",
    "data = data[['v1', 'v2']]  # keep only label and message\n",
    "data.columns = ['label', 'message']  # rename columns\n",
    "\n",
    "# Check the first rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60a7ef-e93e-4324-a7c6-7554e392a0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbbb202-7b56-44d9-aef0-5665d4411417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a6b0e3-2196-4408-8f9c-680450a5330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"label\"] = encoder.fit_transform(data[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd560108-028a-4438-97b1-e4a9a08c23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cde113-9fd6-45a7-8495-705c71de5020",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817290ca-090a-4b2d-b381-628b8d36451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28228508-980f-4cf0-969b-4e28c480f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98edf3-35f9-4703-b031-4da4bbe9a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28b7fb-a978-4c28-a785-2adea32bd87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA\n",
    "data[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d717edb3-e63d-4514-94ac-0bb2dd8ec224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(data[\"label\"].value_counts(),labels = [\"ham\",\"spam\"],autopct = \"%0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d5511-cff0-4e3b-97fc-1ac6bfa8c689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0a0bc-1898-4714-9dd5-5f3c754337d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5d043-4fb1-4883-96dc-ad7609cb9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"num_characters\"] = data['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c48bf32-4f1c-46e0-966f-0722271267d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35281c36-5b26-460e-87ca-b6f248986faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "data[\"num_words\"] = data['message'].apply(lambda x: len(word_tokenize(x)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f81e61-662a-47aa-8281-f93e0cc37c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "data[\"num_sentences\"] = data['message'].apply(lambda x: len(sent_tokenize(x)))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c821eb-72ce-4a74-bf08-b10d893115ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c0d245-f866-4461-a37a-9fc6b61a8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ham\n",
    "data[data[\"label\"] == 0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3216ccb5-fc7c-440d-84f4-5b842fb3a98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam\n",
    "data[data[\"label\"] == 1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7276c934-d2f2-41f7-8c15-50beb660467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad77f89-c6fd-47cf-9872-fbcb90651cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.histplot(data[data[\"label\"] == 0][\"num_characters\"], kde=False)\n",
    "sns.histplot(data[data[\"label\"] == 1][\"num_characters\"], color=\"red\", kde=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e49eb5-f671-46c0-b894-7536046a4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data,hue = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab8c89-5b96-45d7-874f-74c207a1ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only numeric columns\n",
    "corr = data.select_dtypes(include=[\"number\"]).corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef765a-c1fd-44a1-8d50-4ff1b358a696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# Initialize stemmer and stopwords\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess a single string of text:\n",
    "    1. Lowercase\n",
    "    2. Tokenize using nltk.word_tokenize\n",
    "    3. Remove non-alphanumeric tokens\n",
    "    4. Remove stopwords\n",
    "    5. Apply stemming\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 3 & 4. Remove non-alphanumeric tokens and stopwords\n",
    "    clean_tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    \n",
    "    # 5. Stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in clean_tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "preprocess_text(\"Hi how are you Nitish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3e6f33-9662-441b-9c33-36e6503d4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"transformed_text\"] = data['message'].apply(preprocess_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c64268b-e7a4-49eb-ac57-7e3ad66bb1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter label == 1\n",
    "texts = data[data[\"label\"] == 1][\"transformed_text\"]\n",
    "\n",
    "# Join list of tokens into a single string\n",
    "spam_text = \" \".join([\" \".join(tokens) for tokens in texts if isinstance(tokens, list)])\n",
    "\n",
    "# Generate WordCloud\n",
    "spam_wc = WordCloud(width=400, height=400, background_color='white').generate(spam_text)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(spam_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud for label = 1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2264ad-dfb9-45d3-a489-e0a1a27e216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter label == 0\n",
    "texts = data[data[\"label\"] == 0][\"transformed_text\"]\n",
    "\n",
    "# Join list of tokens into a single string\n",
    "ham_text = \" \".join([\" \".join(tokens) for tokens in texts if isinstance(tokens, list)])\n",
    "\n",
    "# Generate WordCloud\n",
    "ham_wc = WordCloud(width=400, height=400, background_color='white').generate(spam_text)\n",
    "\n",
    "# Display\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(spam_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WordCloud for label = 0\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563a660-81ab-4325-9ce1-ca36c5b1e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_top_words(df, label, top_n=30):\n",
    "    texts = df[df[\"label\"] == label][\"transformed_text\"]\n",
    "    word_counts = {}\n",
    "    for tokens in texts:\n",
    "        if isinstance(tokens, list):\n",
    "            for word in tokens:\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "    # Sort and return top N words\n",
    "    return sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "# Get top 30 words for spam and ham\n",
    "top_spam = get_top_words(data, label=1)\n",
    "top_ham = get_top_words(data, label=0)\n",
    "\n",
    "# Separate words and counts\n",
    "spam_words, spam_counts = zip(*top_spam)\n",
    "ham_words, ham_counts = zip(*top_ham)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Spam barplot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(spam_words, spam_counts, color='red')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 Words in Spam Messages\")\n",
    "\n",
    "# Ham barplot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(ham_words, ham_counts, color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top 30 Words in Ham Messages\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d25bb6-845d-473c-851f-5c0e70078a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2cea6-10f5-4df2-9e20-13e4315ea895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data\n",
    "X = data['transformed_text'].apply(lambda x: ' '.join(x))  # join token lists to strings if needed\n",
    "y = data['label']\n",
    "\n",
    "# 2. Convert text to feature vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert to dense array for GaussianNB (needs dense input)\n",
    "X_dense = X_vect.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc8731-e1b1-4056-8742-c9a8a43b781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)\n",
    "X_train_dense, X_test_dense = train_test_split(X_dense, test_size=0.2, random_state=42)  # for GaussianNB\n",
    "\n",
    "# 4. Initialize models\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"GaussianNB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# 5. Train and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    # Use dense input for GaussianNB\n",
    "    if name == \"GaussianNB\":\n",
    "        model.fit(X_train_dense, y_train)\n",
    "        y_pred = model.predict(X_test_dense)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cf = confusion_matrix(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751d0c5-7d78-4a77-9676-66c6cabb5f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "\n",
    "# 1. Prepare data\n",
    "X = data['transformed_text'].apply(lambda x: ' '.join(x))  # join token lists to strings if needed\n",
    "y = data['label']\n",
    "\n",
    "# 2. Convert text to TF-IDF feature vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_vect = vectorizer.fit_transform(X)\n",
    "\n",
    "# Convert to dense array for GaussianNB\n",
    "X_dense = X_vect.toarray()\n",
    "\n",
    "# 3. Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect, y, test_size=0.2, random_state=42)\n",
    "X_train_dense, X_test_dense = train_test_split(X_dense, test_size=0.2, random_state=42)  # for GaussianNB\n",
    "\n",
    "# 4. Initialize models\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"GaussianNB\": GaussianNB()\n",
    "}\n",
    "\n",
    "# 5. Train and evaluate\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    \n",
    "    # Use dense input for GaussianNB\n",
    "    if name == \"GaussianNB\":\n",
    "        model.fit(X_train_dense, y_train)\n",
    "        y_pred = model.predict(X_test_dense)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cf = confusion_matrix(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e72c58-16ff-4e87-9ca8-8eca0cff0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "\n",
    "# ------------- Prepare data ----------------\n",
    "X = data['transformed_text'].apply(lambda x: ' '.join(x))\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "X_train_dense = X_train_tfidf.toarray()  # For models that need dense input\n",
    "X_test_dense = X_test_tfidf.toarray()\n",
    "\n",
    "# ------------- Define models ----------------\n",
    "models = {\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
    "    \"SVC\": SVC(kernel='linear', probability=True),\n",
    "    \"KNeighbors\": KNeighborsClassifier(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# ------------- Train & evaluate ----------------\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name == \"GaussianNB\":\n",
    "        model.fit(X_train_dense, y_train)\n",
    "        y_pred = model.predict(X_test_dense)\n",
    "    else:\n",
    "        model.fit(X_train_tfidf, y_train)\n",
    "        y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred)\n",
    "    results.append({\"Model\": name, \"Accuracy\": acc, \"Precision\": prec})\n",
    "\n",
    "# Convert results to DataFrame\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Top 5 models based on precision\n",
    "top5_precision = results_df.sort_values(by=\"Precision\", ascending=False).head(5)\n",
    "print(\"Top 5 Models Based on Precision:\\n\", top5_precision)\n",
    "\n",
    "# Optional: show full metrics for top 5\n",
    "for name in top5_precision['Model']:\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    if name == \"GaussianNB\":\n",
    "        y_pred = models[name].predict(X_test_dense)\n",
    "    else:\n",
    "        y_pred = models[name].predict(X_test_tfidf)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15520e84-a898-4333-96ac-f4bdc534a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "\n",
    "# Define the top models\n",
    "top_models = [\n",
    "    ('MultinomialNB', MultinomialNB()),\n",
    "    ('BernoulliNB', BernoulliNB()),\n",
    "    ('ExtraTrees', ExtraTreesClassifier(n_estimators=200, random_state=42)),\n",
    "    ('RandomForest', RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "]\n",
    "\n",
    "# Create a soft voting ensemble\n",
    "ensemble = VotingClassifier(estimators=top_models, voting='soft')\n",
    "\n",
    "# Fit ensemble on training data\n",
    "ensemble.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = ensemble.predict(X_test_tfidf)\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "cf = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Ensemble Model Metrics:\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Confusion Matrix:\\n\", cf)\n",
    "print(\"Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e60b7-390c-4add-a31f-24b09b3d6678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# Base models\n",
    "base_models = [\n",
    "    ('MultinomialNB', MultinomialNB()),\n",
    "    ('BernoulliNB', BernoulliNB()),\n",
    "    ('ExtraTrees', ExtraTreesClassifier(n_estimators=200, random_state=42)),\n",
    "    ('RandomForest', RandomForestClassifier(n_estimators=200, random_state=42))\n",
    "]\n",
    "\n",
    "# Meta-model as RandomForest\n",
    "meta_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Stacking ensemble\n",
    "stacking_ensemble_rf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "# Fit ensemble\n",
    "stacking_ensemble_rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_stack_rf = stacking_ensemble_rf.predict(X_test_tfidf)\n",
    "\n",
    "# Metrics\n",
    "acc_stack_rf = accuracy_score(y_test, y_pred_stack_rf)\n",
    "prec_stack_rf = precision_score(y_test, y_pred_stack_rf)\n",
    "cf_stack_rf = confusion_matrix(y_test, y_pred_stack_rf)\n",
    "report_stack_rf = classification_report(y_test, y_pred_stack_rf)\n",
    "\n",
    "print(\"Stacking Ensemble with RandomForest as Meta-Model:\")\n",
    "print(\"Accuracy:\", acc_stack_rf)\n",
    "print(\"Precision:\", prec_stack_rf)\n",
    "print(\"Confusion Matrix:\\n\", cf_stack_rf)\n",
    "print(\"Classification Report:\\n\", report_stack_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b370857-8753-4ed6-8e87-c0d221aca437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
